##### SENet概述:
采用一种全新的`特征重标定`策略，通过学习的方式来自动获取到每个特征的重要程度，根据这个重要程度来提升有用的特征并抑制对当前任务作用不大的特征，通俗地说，SENet的核心思想在于根据网络的loss来去学习特征权重，使得有效的feature map权重大，无效或者效果小的feature map权重小来使训练模型达到更好的效果，因此SE block在原本的网络中不可避免地增加了一些参数量和计算量，但是效果的提升还能接受。

### Squeeze-and-Excitation Networks 结构
![](https://img-blog.csdn.net/20180423230918755)

* __首先是Squeeze操作，顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，这个实数在某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野，这一点在很多任务中都是非常有用的。__  
* __其次是Excitation操作，它是一个类似于循环神经网络中门的机制，通过参数w来为每个特征通道生成权重，其中参数w被学习用来显示地建模特征通道的相关性。__  
* __最后一个是Reweight操作，将Excitation的输出的权重看做是进行过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原石特征的重标定。__

__1. 图中的Ftr运算在原文中只是一个普通的卷积操作，vc代表第c个卷积核，xs表示第s个输入。__  
![](https://img-blog.csdn.net/20180423232510071)  
__2. Squeeze操作，公式非常的简单，就是一个global average pooling。__  
![](https://img-blog.csdn.net/20180423232814869)  
__3. Excitation操作，Squeeze输出的为z，先用一个w1乘以z，就是一个全连接层的操作，w1的维度为`c\*c/r`，r是一个缩放参数，在原论文中取的是16，这个参数的目的是为了减少channel个数从而降低计算量，因为z的维度是1\*1\*c，所以w1z的结果就是1\*1\*c/r，然后经过一个ReLu层，输出的维度不变；然后再和w2相乘，和w2相乘也是一个全连接层的操作，w2的维度为c\*c/r，因此输出的维度是1\*1\*c，最后经过sigmoid函数，得到s，就是每一个通道的权重。`先经过w1降维，减少计算量，之后经过ReLu层，之后再与w2相乘，得到1\*1\*c的输出，再通过sigmoid映射到0-1之间表示权重.`__
![](https://img-blog.csdn.net/20180423233041069)  
__4. 得到s之后，就可以对原来的tensor U进行操作了，sc是一个数，也就是权重，因此相当于把uc矩阵中的每个值都乘以sc。__  
![](https://img-blog.csdn.net/20180423233204608)  

#### SENet在具体网络中的应用.
![](https://img-blog.csdn.net/20180423233511251)

__这里我们使用一个global average pooling层来当作Squeeze操作，之后两个全连接层组成一个Bottleneck结构去建模通道间的相关性，并输出和输入特征同样数目的权重。为什么要使用两个全连接层，原因在于(1)具有更多的非线性，可以更好地拟合通道间复杂的相关性(2)极大地减少参数量和计算量，最后通过一个sigmoid层获得0-1的权重。__

#### SENet计算量比较:
![](https://img-blog.csdn.net/20180423233944806)
