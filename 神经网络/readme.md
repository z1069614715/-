### 杰仔之手动搭建神经网络项目

##### 2019.12.6 更新记录
__1. 该版本没有增加激活函数的反向传播，暂时不能使用激活函数。__  
__2. 该版本一定需要使用bias，因为在反向传播的函数，一定会计算bias，如果不使用bias会报错。__  
__3. 只调试过3层的神经网络，loss能顺利下降，等完善激活函数的反向传播后，就用该神经网络进行mnist的数据集测试。__  
__4. 该版本调用一次forward和一次back_forward为一次训练过程。__  

##### 2019.12.11 更新记录
__1. 优化add_layer函数里面的添加权重操作。__  
__2. 优化权重更新操作。__  
__3. 新增Activation.py文件，添加激活函数，暂时支持relu，sigmoid，tanh激活函数。__  
__4. 把forward函数和back_forward函数合并为fit函数，调用fit传上相关参数即可进行训练。__  
__5. 新增plt_loss函数，可以画出训练时候的loss值变化__  
__6. 新增训练时候的callback，对应在fit的参数里面，暂时只支持SaveBest，保存最好的模型参数和loss。__  

##### 2019.12.17 更新记录
__1. Activation类增加softmax层。__  
__2. 新增sgd, momentum, adam优化器。__  
__3. 新增交叉熵loss前向传播和反向传播函数，callback支持计算分类精度accuracy，计算多分类时的验证集精度。__
__4. 对于与keras搭建相同的模型，训练的收敛速度稍微差keras一点，当然训练速度是不可能与其相比，这里讨论的是收敛速度。__

## 暂时应该不作更新，因为再更新也只是添加功能，与神经网络原理已经没多大关系了，所以就不作更新了。
